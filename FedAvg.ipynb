{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "02lAmCwP-4rg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, Dataset, DataLoader, ConcatDataset, Subset\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(77)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2JCGAjx1fuS",
        "outputId": "803a8732-44a0-4151-d446-8197e2ec19ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0c00931190>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dlS-Yz-7-_u0"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                    nn.BatchNorm2d(32))\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(64),\n",
        "                                    nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer3 = nn.Sequential(nn.Linear(2304, 128),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(p = 0.5))\n",
        "        self.fc = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layer3(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.vgg19(pretrained=True)\n",
        "\n",
        "\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.classifier\n",
        "\n",
        "#setting a new classifier from scratch\n",
        "model.classifier = nn.Sequential(model.classifier,\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(p = 0.5),\n",
        "                                 nn.Linear(1000, 10))\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "model.classifier[0][0].weight.requires_grad = False\n",
        "\n",
        "\n",
        "summary(model.cuda(), (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOBXxuGhHPjC",
        "outputId": "33f5ad5f-b047-4917-867e-fe04840c4b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-2         [-1, 64, 224, 224]               0\n",
            "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
            "              ReLU-4         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
            "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-7        [-1, 128, 112, 112]               0\n",
            "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
            "              ReLU-9        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
            "             ReLU-12          [-1, 256, 56, 56]               0\n",
            "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-14          [-1, 256, 56, 56]               0\n",
            "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-18          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-19          [-1, 256, 28, 28]               0\n",
            "           Conv2d-20          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-21          [-1, 512, 28, 28]               0\n",
            "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-23          [-1, 512, 28, 28]               0\n",
            "           Conv2d-24          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-25          [-1, 512, 28, 28]               0\n",
            "           Conv2d-26          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-27          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-28          [-1, 512, 14, 14]               0\n",
            "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-30          [-1, 512, 14, 14]               0\n",
            "           Conv2d-31          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-32          [-1, 512, 14, 14]               0\n",
            "           Conv2d-33          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-34          [-1, 512, 14, 14]               0\n",
            "           Conv2d-35          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-36          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-37            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-38            [-1, 512, 7, 7]               0\n",
            "           Linear-39                 [-1, 4096]     102,764,544\n",
            "             ReLU-40                 [-1, 4096]               0\n",
            "          Dropout-41                 [-1, 4096]               0\n",
            "           Linear-42                 [-1, 4096]      16,781,312\n",
            "             ReLU-43                 [-1, 4096]               0\n",
            "          Dropout-44                 [-1, 4096]               0\n",
            "           Linear-45                 [-1, 1000]       4,097,000\n",
            "             ReLU-46                 [-1, 1000]               0\n",
            "          Dropout-47                 [-1, 1000]               0\n",
            "           Linear-48                   [-1, 10]          10,010\n",
            "================================================================\n",
            "Total params: 143,677,250\n",
            "Trainable params: 20,888,322\n",
            "Non-trainable params: 122,788,928\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 238.70\n",
            "Params size (MB): 548.09\n",
            "Estimated Total Size (MB): 787.36\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T_PYLLZm_MfD"
      },
      "outputs": [],
      "source": [
        "from IPython.utils.text import indent\n",
        "def train_on_client(client_data, ind, model, criterion, optimizer, num_epochs, device, bs):\n",
        "    train_loader = DataLoader(client_data, batch_size=bs, shuffle=True)\n",
        "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        loss = 0.0\n",
        "        \n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss += loss.item() #* images.size(0)\n",
        "            #print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss = loss / len(train_loader)\n",
        "        print(f\"Client {ind} Training: Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BQdnbTRp_PeB"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the server model\n",
        "server_model = CNN().to(device)\n",
        "\n",
        "# Define the hyperparameters\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 7\n",
        "batch_size = 32\n",
        "num_rounds = 5\n",
        "num_clients = 4\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncG8K9jN_S1y",
        "outputId": "34086f56-958e-4efd-a843-aa6e27292367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.\n",
        "                                transforms.ToTensor(), \n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# partitioning data across clients\n",
        "client_data = torch.utils.data.random_split(train_dataset, [len(train_dataset) // num_clients] * num_clients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPDlBy8l_WSb",
        "outputId": "cd13703c-1f4e-4d92-8c66-4fbd6f8f6897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Round 1 ----------\n",
            "Client 1 Training: Epoch 1/7, Loss: 0.0071\n",
            "Client 1 Training: Epoch 2/7, Loss: 0.0079\n",
            "Client 1 Training: Epoch 3/7, Loss: 0.0071\n",
            "Client 1 Training: Epoch 4/7, Loss: 0.0068\n",
            "Client 1 Training: Epoch 5/7, Loss: 0.0047\n",
            "Client 1 Training: Epoch 6/7, Loss: 0.0033\n",
            "Client 1 Training: Epoch 7/7, Loss: 0.0041\n",
            "Client 2 Training: Epoch 1/7, Loss: 0.0085\n",
            "Client 2 Training: Epoch 2/7, Loss: 0.0061\n",
            "Client 2 Training: Epoch 3/7, Loss: 0.0049\n",
            "Client 2 Training: Epoch 4/7, Loss: 0.0039\n",
            "Client 2 Training: Epoch 5/7, Loss: 0.0082\n",
            "Client 2 Training: Epoch 6/7, Loss: 0.0057\n",
            "Client 2 Training: Epoch 7/7, Loss: 0.0040\n",
            "Client 3 Training: Epoch 1/7, Loss: 0.0051\n",
            "Client 3 Training: Epoch 2/7, Loss: 0.0068\n",
            "Client 3 Training: Epoch 3/7, Loss: 0.0045\n",
            "Client 3 Training: Epoch 4/7, Loss: 0.0057\n",
            "Client 3 Training: Epoch 5/7, Loss: 0.0061\n",
            "Client 3 Training: Epoch 6/7, Loss: 0.0049\n",
            "Client 3 Training: Epoch 7/7, Loss: 0.0035\n",
            "Client 4 Training: Epoch 1/7, Loss: 0.0074\n",
            "Client 4 Training: Epoch 2/7, Loss: 0.0049\n",
            "Client 4 Training: Epoch 3/7, Loss: 0.0066\n",
            "Client 4 Training: Epoch 4/7, Loss: 0.0069\n",
            "Client 4 Training: Epoch 5/7, Loss: 0.0054\n",
            "Client 4 Training: Epoch 6/7, Loss: 0.0051\n",
            "Client 4 Training: Epoch 7/7, Loss: 0.0043\n",
            "---------- Round 2 ----------\n",
            "Client 1 Training: Epoch 1/7, Loss: 0.0047\n",
            "Client 1 Training: Epoch 2/7, Loss: 0.0049\n",
            "Client 1 Training: Epoch 3/7, Loss: 0.0045\n",
            "Client 1 Training: Epoch 4/7, Loss: 0.0036\n",
            "Client 1 Training: Epoch 5/7, Loss: 0.0053\n",
            "Client 1 Training: Epoch 6/7, Loss: 0.0027\n",
            "Client 1 Training: Epoch 7/7, Loss: 0.0038\n",
            "Client 2 Training: Epoch 1/7, Loss: 0.0087\n",
            "Client 2 Training: Epoch 2/7, Loss: 0.0060\n",
            "Client 2 Training: Epoch 3/7, Loss: 0.0040\n",
            "Client 2 Training: Epoch 4/7, Loss: 0.0036\n",
            "Client 2 Training: Epoch 5/7, Loss: 0.0034\n",
            "Client 2 Training: Epoch 6/7, Loss: 0.0037\n",
            "Client 2 Training: Epoch 7/7, Loss: 0.0039\n",
            "Client 3 Training: Epoch 1/7, Loss: 0.0074\n",
            "Client 3 Training: Epoch 2/7, Loss: 0.0061\n",
            "Client 3 Training: Epoch 3/7, Loss: 0.0037\n",
            "Client 3 Training: Epoch 4/7, Loss: 0.0058\n",
            "Client 3 Training: Epoch 5/7, Loss: 0.0026\n",
            "Client 3 Training: Epoch 6/7, Loss: 0.0031\n",
            "Client 3 Training: Epoch 7/7, Loss: 0.0013\n",
            "Client 4 Training: Epoch 1/7, Loss: 0.0037\n",
            "Client 4 Training: Epoch 2/7, Loss: 0.0045\n",
            "Client 4 Training: Epoch 3/7, Loss: 0.0069\n",
            "Client 4 Training: Epoch 4/7, Loss: 0.0023\n",
            "Client 4 Training: Epoch 5/7, Loss: 0.0026\n",
            "Client 4 Training: Epoch 6/7, Loss: 0.0044\n",
            "Client 4 Training: Epoch 7/7, Loss: 0.0063\n",
            "---------- Round 3 ----------\n",
            "Client 1 Training: Epoch 1/7, Loss: 0.0028\n",
            "Client 1 Training: Epoch 2/7, Loss: 0.0043\n",
            "Client 1 Training: Epoch 3/7, Loss: 0.0036\n",
            "Client 1 Training: Epoch 4/7, Loss: 0.0016\n",
            "Client 1 Training: Epoch 5/7, Loss: 0.0046\n",
            "Client 1 Training: Epoch 6/7, Loss: 0.0037\n",
            "Client 1 Training: Epoch 7/7, Loss: 0.0016\n",
            "Client 2 Training: Epoch 1/7, Loss: 0.0082\n",
            "Client 2 Training: Epoch 2/7, Loss: 0.0058\n",
            "Client 2 Training: Epoch 3/7, Loss: 0.0033\n",
            "Client 2 Training: Epoch 4/7, Loss: 0.0034\n",
            "Client 2 Training: Epoch 5/7, Loss: 0.0026\n",
            "Client 2 Training: Epoch 6/7, Loss: 0.0037\n",
            "Client 2 Training: Epoch 7/7, Loss: 0.0014\n",
            "Client 3 Training: Epoch 1/7, Loss: 0.0054\n",
            "Client 3 Training: Epoch 2/7, Loss: 0.0044\n",
            "Client 3 Training: Epoch 3/7, Loss: 0.0042\n",
            "Client 3 Training: Epoch 4/7, Loss: 0.0017\n",
            "Client 3 Training: Epoch 5/7, Loss: 0.0023\n",
            "Client 3 Training: Epoch 6/7, Loss: 0.0055\n",
            "Client 3 Training: Epoch 7/7, Loss: 0.0050\n",
            "Client 4 Training: Epoch 1/7, Loss: 0.0040\n",
            "Client 4 Training: Epoch 2/7, Loss: 0.0030\n",
            "Client 4 Training: Epoch 3/7, Loss: 0.0022\n",
            "Client 4 Training: Epoch 4/7, Loss: 0.0035\n",
            "Client 4 Training: Epoch 5/7, Loss: 0.0046\n",
            "Client 4 Training: Epoch 6/7, Loss: 0.0034\n",
            "Client 4 Training: Epoch 7/7, Loss: 0.0025\n",
            "---------- Round 4 ----------\n",
            "Client 1 Training: Epoch 1/7, Loss: 0.0034\n",
            "Client 1 Training: Epoch 2/7, Loss: 0.0038\n",
            "Client 1 Training: Epoch 3/7, Loss: 0.0040\n",
            "Client 1 Training: Epoch 4/7, Loss: 0.0016\n",
            "Client 1 Training: Epoch 5/7, Loss: 0.0008\n",
            "Client 1 Training: Epoch 6/7, Loss: 0.0024\n",
            "Client 1 Training: Epoch 7/7, Loss: 0.0029\n",
            "Client 2 Training: Epoch 1/7, Loss: 0.0034\n",
            "Client 2 Training: Epoch 2/7, Loss: 0.0033\n",
            "Client 2 Training: Epoch 3/7, Loss: 0.0030\n",
            "Client 2 Training: Epoch 4/7, Loss: 0.0045\n",
            "Client 2 Training: Epoch 5/7, Loss: 0.0022\n",
            "Client 2 Training: Epoch 6/7, Loss: 0.0027\n",
            "Client 2 Training: Epoch 7/7, Loss: 0.0010\n",
            "Client 3 Training: Epoch 1/7, Loss: 0.0042\n",
            "Client 3 Training: Epoch 2/7, Loss: 0.0084\n",
            "Client 3 Training: Epoch 3/7, Loss: 0.0037\n",
            "Client 3 Training: Epoch 4/7, Loss: 0.0036\n",
            "Client 3 Training: Epoch 5/7, Loss: 0.0008\n",
            "Client 3 Training: Epoch 6/7, Loss: 0.0021\n",
            "Client 3 Training: Epoch 7/7, Loss: 0.0010\n",
            "Client 4 Training: Epoch 1/7, Loss: 0.0044\n",
            "Client 4 Training: Epoch 2/7, Loss: 0.0032\n",
            "Client 4 Training: Epoch 3/7, Loss: 0.0046\n",
            "Client 4 Training: Epoch 4/7, Loss: 0.0018\n",
            "Client 4 Training: Epoch 5/7, Loss: 0.0022\n",
            "Client 4 Training: Epoch 6/7, Loss: 0.0005\n",
            "Client 4 Training: Epoch 7/7, Loss: 0.0033\n",
            "---------- Round 5 ----------\n",
            "Client 1 Training: Epoch 1/7, Loss: 0.0059\n",
            "Client 1 Training: Epoch 2/7, Loss: 0.0031\n",
            "Client 1 Training: Epoch 3/7, Loss: 0.0028\n",
            "Client 1 Training: Epoch 4/7, Loss: 0.0042\n",
            "Client 1 Training: Epoch 5/7, Loss: 0.0048\n",
            "Client 1 Training: Epoch 6/7, Loss: 0.0018\n",
            "Client 1 Training: Epoch 7/7, Loss: 0.0032\n",
            "Client 2 Training: Epoch 1/7, Loss: 0.0019\n",
            "Client 2 Training: Epoch 2/7, Loss: 0.0061\n",
            "Client 2 Training: Epoch 3/7, Loss: 0.0066\n",
            "Client 2 Training: Epoch 4/7, Loss: 0.0025\n",
            "Client 2 Training: Epoch 5/7, Loss: 0.0014\n",
            "Client 2 Training: Epoch 6/7, Loss: 0.0012\n",
            "Client 2 Training: Epoch 7/7, Loss: 0.0061\n",
            "Client 3 Training: Epoch 1/7, Loss: 0.0030\n",
            "Client 3 Training: Epoch 2/7, Loss: 0.0043\n",
            "Client 3 Training: Epoch 3/7, Loss: 0.0026\n",
            "Client 3 Training: Epoch 4/7, Loss: 0.0027\n",
            "Client 3 Training: Epoch 5/7, Loss: 0.0016\n",
            "Client 3 Training: Epoch 6/7, Loss: 0.0006\n",
            "Client 3 Training: Epoch 7/7, Loss: 0.0025\n",
            "Client 4 Training: Epoch 1/7, Loss: 0.0025\n",
            "Client 4 Training: Epoch 2/7, Loss: 0.0026\n",
            "Client 4 Training: Epoch 3/7, Loss: 0.0025\n",
            "Client 4 Training: Epoch 4/7, Loss: 0.0040\n",
            "Client 4 Training: Epoch 5/7, Loss: 0.0030\n",
            "Client 4 Training: Epoch 6/7, Loss: 0.0022\n",
            "Client 4 Training: Epoch 7/7, Loss: 0.0015\n"
          ]
        }
      ],
      "source": [
        "clients = [[],[],[],[]]\n",
        "\n",
        "for round in range(num_rounds):\n",
        "\n",
        "    print(f\"---------- Round {round+1} ----------\")\n",
        "    # Aggregate client models on the server\n",
        "    server_model.train()\n",
        "    server_model.zero_grad()\n",
        "    \n",
        "    for client_idx in range(num_clients):\n",
        "        client_model = CNN().to(device)\n",
        "        client_model.load_state_dict(server_model.state_dict())\n",
        "        optimizer = torch.optim.Adam(client_model.parameters(), lr = lr, weight_decay = weight_decay)  \n",
        "        \n",
        "        # Train on the client's local dataset\n",
        "        train_on_client(client_data[client_idx], client_idx+1, client_model, \n",
        "                        criterion, optimizer, num_epochs, device, batch_size)\n",
        "        clients[client_idx] = client_model\n",
        "\n",
        "    sd_server = server_model.state_dict()\n",
        "    sd_clients = [cl.state_dict() for cl in clients]\n",
        "    for key in sd_server:\n",
        "      for sd_client in sd_clients:\n",
        "        sd_server[key] += sd_client[key]\n",
        "      sd_server[key] = (sd_server[key] / num_clients).float()\n",
        "    \n",
        "    server_model.load_state_dict(sd_server)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 16)\n",
        "loss_test = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i, (input, target) in enumerate(test_dataloader):\n",
        "\n",
        "        target = target.to(device)\n",
        "        input = input.to(device)\n",
        "\n",
        "        # compute output\n",
        "        output = server_model(input)\n",
        "        loss = criterion(output, target)\n",
        "        loss_test.append(loss.item())\n",
        "\n",
        "        total += target.size(0)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        \n",
        "  print('Accuracy on the test images: {} %'.format(100 * correct / total)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sX-F5TNgh0R",
        "outputId": "3922a090-3358-421e-ef7e-04d7c52b8c88"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on the test images: 68.09 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "torch.save(server_model.state_dict(),'/content/drive/MyDrive/PrivacyModels/FedAvg-model.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbRooSZN4UBf",
        "outputId": "0f29df97-f9c8-4f0a-a831-7c5899c19f96"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/PrivacyModels/FedAvg-model.pt'))\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "wu4xyIry43O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison with Global Model"
      ],
      "metadata": {
        "id": "wj7StokK6ufo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_data = train_dataset\n",
        "global_dataloader = DataLoader(global_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 25\n",
        "batch_size = 32\n",
        "\n",
        "global_model = CNN().to(device)\n",
        "optimizer = torch.optim.Adam(global_model.parameters(), lr = lr, weight_decay = weight_decay)  \n",
        "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)\n",
        "\n",
        "print(\"Global Model Training:\")\n",
        "for epoch in range(num_epochs):\n",
        "  loss_global = 0.0\n",
        "\n",
        "  for i, (input, target) in enumerate(global_dataloader):\n",
        "          \n",
        "          target = target.to(device)\n",
        "          input = input.to(device)\n",
        "\n",
        "          # compute output\n",
        "          output = global_model(input)\n",
        "          loss = criterion(output, target)\n",
        "          loss_global += loss.item()\n",
        "          # backpropagate\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          #print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_dataloader), loss.item()))\n",
        "  epoch_loss = loss_global / len(global_dataloader)\n",
        "  scheduler.step()\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfNNM4536z8P",
        "outputId": "a51ce50a-2d2f-4ace-de7a-f3827fa6c1a4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Model Training:\n",
            "Epoch 1/25, Loss: 1.4133\n",
            "Epoch 2/25, Loss: 1.1114\n",
            "Epoch 3/25, Loss: 0.9787\n",
            "Epoch 4/25, Loss: 0.8942\n",
            "Epoch 5/25, Loss: 0.8243\n",
            "Epoch 6/25, Loss: 0.7614\n",
            "Epoch 7/25, Loss: 0.7120\n",
            "Epoch 8/25, Loss: 0.6673\n",
            "Epoch 9/25, Loss: 0.6378\n",
            "Epoch 10/25, Loss: 0.5973\n",
            "Epoch 11/25, Loss: 0.5663\n",
            "Epoch 12/25, Loss: 0.5378\n",
            "Epoch 13/25, Loss: 0.5196\n",
            "Epoch 14/25, Loss: 0.4897\n",
            "Epoch 15/25, Loss: 0.4698\n",
            "Epoch 16/25, Loss: 0.4557\n",
            "Epoch 17/25, Loss: 0.4324\n",
            "Epoch 18/25, Loss: 0.4206\n",
            "Epoch 19/25, Loss: 0.3964\n",
            "Epoch 20/25, Loss: 0.3840\n",
            "Epoch 21/25, Loss: 0.3733\n",
            "Epoch 22/25, Loss: 0.3630\n",
            "Epoch 23/25, Loss: 0.3535\n",
            "Epoch 24/25, Loss: 0.3377\n",
            "Epoch 25/25, Loss: 0.3287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i, (input, target) in enumerate(test_dataloader):\n",
        "\n",
        "        target = target.to(device)\n",
        "        input = input.to(device)\n",
        "\n",
        "        # compute output\n",
        "        output = global_model(input)\n",
        "        loss = criterion(output, target)\n",
        "      \n",
        "        total += target.size(0)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        \n",
        "  print('Accuracy of Global Model the test images: {} %'.format(100 * correct / total)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL86jlqL8xoI",
        "outputId": "40356d04-bf33-44cc-ea9b-3b59ba79bbb6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Global Model the test images: 69.21 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see, the federated model has an accuracy close to the global model, even though we only trained the model for 5 rounds. In practice clients are trained in parallel and therefore takes a much lesser time. Also, asynchronous implementation of FedAvg is possible as well, which is not the desired here. Note that this was just a relatively simple model and more engineering on the model could yeild better results.**\n",
        "\n",
        "(I tried the FedAvg algorithm for fine-tuning the VGG19 and VGG16 models as well; however their results were disappointing! (not sure about the reason behind this.)"
      ],
      "metadata": {
        "id": "n904196rCe2Y"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}